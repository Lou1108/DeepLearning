{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-21T09:20:40.023374Z",
     "start_time": "2025-05-21T09:20:40.020926Z"
    }
   },
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T09:27:21.610813Z",
     "start_time": "2025-05-21T09:27:21.606168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_from_folder(folder_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.endswith('.h5'):\n",
    "            if 'rest' in fname:\n",
    "                label = 0\n",
    "            elif 'math' in fname or 'story' in fname:\n",
    "                label = 1\n",
    "            elif 'memory' in fname:\n",
    "                label = 2\n",
    "            elif 'motor' in fname:\n",
    "                label = 3\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if ' ' in fname:\n",
    "                dataset_name = ' '.join(fname.split(' ')[:-1])\n",
    "            elif '_' in fname:\n",
    "                dataset_name = '_'.join(fname.split('_')[:-1])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            with h5py.File(os.path.join(folder_path, fname), 'r') as f:\n",
    "                matrix = np.array(f[dataset_name])\n",
    "                data.append(matrix)\n",
    "                labels.append(label)\n",
    "    return np.array(data), np.array(labels)"
   ],
   "id": "5dbabf48dcec70d8",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T09:27:43.894167Z",
     "start_time": "2025-05-21T09:27:37.526568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_path = \"../dataset/Final Project data/Cross/\"\n",
    "train_folder = os.path.join(base_path, \"train\")\n",
    "test_folders = [os.path.join(base_path, f\"test{i}\") for i in range(1, 4)]\n",
    "\n",
    "X_train, y_train = load_data_from_folder(train_folder)\n",
    "\n",
    "X_test_list, y_test_list = [], []\n",
    "for test_folder in test_folders:\n",
    "    X_test_part, y_test_part = load_data_from_folder(test_folder)\n",
    "    X_test_list.append(X_test_part)\n",
    "    y_test_list.append(y_test_part)\n",
    "X_test = np.concatenate(X_test_list, axis=0)\n",
    "y_test = np.concatenate(y_test_list, axis=0)\n",
    "\n",
    "print(\"Train data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape, y_test.shape)"
   ],
   "id": "28b1816aac1a0390",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (64, 248, 35624) (64,)\n",
      "Test data shape: (48, 248, 35624) (48,)\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T09:30:42.601008Z",
     "start_time": "2025-05-21T09:30:42.598041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def zscore_normalize(X):\n",
    "    mean = np.mean(X, axis=2, keepdims=True)\n",
    "    std = np.std(X, axis=2, keepdims=True) + 1e-8  # Avoid divide by zero\n",
    "    return (X - mean) / std\n",
    "\n",
    "def downsample(X, factor=8):\n",
    "    return X[:, :, ::factor]"
   ],
   "id": "a1e1fdd05320ae73",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T09:31:28.552357Z",
     "start_time": "2025-05-21T09:30:54.524439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_norm = zscore_normalize(X_train)\n",
    "X_test_norm = zscore_normalize(X_test)\n",
    "\n",
    "#TODO: ask the TA if it's needed or not\n",
    "X_train_ds = downsample(X_train_norm, factor=8)\n",
    "X_test_ds = downsample(X_test_norm, factor=8)\n",
    "\n",
    "X_train_final = X_train_ds[..., np.newaxis]\n",
    "X_test_final = X_test_ds[..., np.newaxis]\n",
    "\n",
    "print(\"Train shape ready for CNN:\", X_train_final.shape)\n",
    "print(\"Test shape ready for CNN:\", X_test_final.shape)"
   ],
   "id": "4984709f2174a899",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape ready for CNN: (64, 248, 4453, 1)\n",
      "Test shape ready for CNN: (48, 248, 4453, 1)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T09:40:18.690889Z",
     "start_time": "2025-05-21T09:40:16.574553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=X_train_final.shape[1:]),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ],
   "id": "24751168f5a77d91",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T09:41:44.396407Z",
     "start_time": "2025-05-21T09:40:33.079005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final, y_train,\n",
    "    batch_size=16,\n",
    "    epochs=30,\n",
    "    validation_data=(X_test_final, y_test),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")"
   ],
   "id": "9ca1dd7456cfa2ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[36]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m early_stop = EarlyStopping(monitor=\u001B[33m'\u001B[39m\u001B[33mval_loss\u001B[39m\u001B[33m'\u001B[39m, patience=\u001B[32m5\u001B[39m, restore_best_weights=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# You can adjust batch_size depending on your RAM (try 8, 16, or 32)\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m history = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_train_final\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test_final\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mearly_stop\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\n\u001B[32m     14\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Utrecht University/Data Visualization/DeepLearning/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    115\u001B[39m filtered_tb = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m117\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Utrecht University/Data Visualization/DeepLearning/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:391\u001B[39m, in \u001B[36mTensorFlowTrainer.fit\u001B[39m\u001B[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[39m\n\u001B[32m    386\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m validation_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._should_eval(\n\u001B[32m    387\u001B[39m     epoch, validation_freq\n\u001B[32m    388\u001B[39m ):\n\u001B[32m    389\u001B[39m     \u001B[38;5;66;03m# Create EpochIterator for evaluation and cache it.\u001B[39;00m\n\u001B[32m    390\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m_eval_epoch_iterator\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m391\u001B[39m         \u001B[38;5;28mself\u001B[39m._eval_epoch_iterator = \u001B[43mTFEpochIterator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    392\u001B[39m \u001B[43m            \u001B[49m\u001B[43mx\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_x\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    393\u001B[39m \u001B[43m            \u001B[49m\u001B[43my\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    394\u001B[39m \u001B[43m            \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_sample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    395\u001B[39m \u001B[43m            \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidation_batch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    396\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdistribute_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdistribute_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    397\u001B[39m \u001B[43m            \u001B[49m\u001B[43msteps_per_execution\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msteps_per_execution\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    398\u001B[39m \u001B[43m            \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    399\u001B[39m \u001B[43m            \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    400\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    401\u001B[39m     val_logs = \u001B[38;5;28mself\u001B[39m.evaluate(\n\u001B[32m    402\u001B[39m         x=val_x,\n\u001B[32m    403\u001B[39m         y=val_y,\n\u001B[32m   (...)\u001B[39m\u001B[32m    409\u001B[39m         _use_cached_eval_dataset=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    410\u001B[39m     )\n\u001B[32m    411\u001B[39m     val_logs = {\n\u001B[32m    412\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mval_\u001B[39m\u001B[33m\"\u001B[39m + name: val \u001B[38;5;28;01mfor\u001B[39;00m name, val \u001B[38;5;129;01min\u001B[39;00m val_logs.items()\n\u001B[32m    413\u001B[39m     }\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Utrecht University/Data Visualization/DeepLearning/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:726\u001B[39m, in \u001B[36mTFEpochIterator.__init__\u001B[39m\u001B[34m(self, distribute_strategy, *args, **kwargs)\u001B[39m\n\u001B[32m    724\u001B[39m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(*args, **kwargs)\n\u001B[32m    725\u001B[39m \u001B[38;5;28mself\u001B[39m._distribute_strategy = distribute_strategy\n\u001B[32m--> \u001B[39m\u001B[32m726\u001B[39m dataset = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdata_adapter\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_tf_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    727\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataset, tf.distribute.DistributedDataset):\n\u001B[32m    728\u001B[39m     dataset = \u001B[38;5;28mself\u001B[39m._distribute_strategy.experimental_distribute_dataset(\n\u001B[32m    729\u001B[39m         dataset\n\u001B[32m    730\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Utrecht University/Data Visualization/DeepLearning/.venv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:235\u001B[39m, in \u001B[36mArrayDataAdapter.get_tf_dataset\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    232\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m shuffle == \u001B[33m\"\u001B[39m\u001B[33mbatch\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    233\u001B[39m     indices_dataset = indices_dataset.map(tf.random.shuffle)\n\u001B[32m--> \u001B[39m\u001B[32m235\u001B[39m dataset = \u001B[43mslice_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindices_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    237\u001B[39m options = tf.data.Options()\n\u001B[32m    238\u001B[39m options.experimental_distribute.auto_shard_policy = (\n\u001B[32m    239\u001B[39m     tf.data.experimental.AutoShardPolicy.DATA\n\u001B[32m    240\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Utrecht University/Data Visualization/DeepLearning/.venv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:191\u001B[39m, in \u001B[36mArrayDataAdapter.get_tf_dataset.<locals>.slice_inputs\u001B[39m\u001B[34m(indices_dataset, inputs)\u001B[39m\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mslice_inputs\u001B[39m(indices_dataset, inputs):\n\u001B[32m    177\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Slice inputs into a Dataset of batches.\u001B[39;00m\n\u001B[32m    178\u001B[39m \n\u001B[32m    179\u001B[39m \u001B[33;03m    Given a Dataset of batch indices and the unsliced inputs,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    189\u001B[39m \u001B[33;03m        A Dataset of input batches matching the batch indices.\u001B[39;00m\n\u001B[32m    190\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m191\u001B[39m     inputs = \u001B[43marray_slicing\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconvert_to_sliceable\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    192\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_backend\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtensorflow\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m    193\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    194\u001B[39m     inputs = tree.lists_to_tuples(inputs)\n\u001B[32m    196\u001B[39m     dataset = tf.data.Dataset.zip(\n\u001B[32m    197\u001B[39m         (indices_dataset, tf.data.Dataset.from_tensors(inputs).repeat())\n\u001B[32m    198\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Utrecht University/Data Visualization/DeepLearning/.venv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/array_slicing.py:458\u001B[39m, in \u001B[36mconvert_to_sliceable\u001B[39m\u001B[34m(arrays, target_backend)\u001B[39m\n\u001B[32m    454\u001B[39m         sliceable_class = NumpySliceable\n\u001B[32m    456\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m sliceable_class(x)\n\u001B[32m--> \u001B[39m\u001B[32m458\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtree\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_structure\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert_single_array\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Utrecht University/Data Visualization/DeepLearning/.venv/lib/python3.11/site-packages/keras/src/tree/tree_api.py:192\u001B[39m, in \u001B[36mmap_structure\u001B[39m\u001B[34m(func, *structures)\u001B[39m\n\u001B[32m    162\u001B[39m \u001B[38;5;129m@keras_export\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mkeras.tree.map_structure\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    163\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmap_structure\u001B[39m(func, *structures):\n\u001B[32m    164\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Maps `func` through given structures.\u001B[39;00m\n\u001B[32m    165\u001B[39m \n\u001B[32m    166\u001B[39m \u001B[33;03m    Examples:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    190\u001B[39m \u001B[33;03m            `assert_same_structure`.\u001B[39;00m\n\u001B[32m    191\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtree_impl\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_structure\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mstructures\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Utrecht University/Data Visualization/DeepLearning/.venv/lib/python3.11/site-packages/keras/src/tree/optree_impl.py:111\u001B[39m, in \u001B[36mmap_structure\u001B[39m\u001B[34m(func, *structures)\u001B[39m\n\u001B[32m    107\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m func(*args)\n\u001B[32m    109\u001B[39m map_func = func_with_check \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(structures) > \u001B[32m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m func\n\u001B[32m--> \u001B[39m\u001B[32m111\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moptree\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtree_map\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    112\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmap_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mstructures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnone_is_leaf\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnamespace\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mkeras\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m    113\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Utrecht University/Data Visualization/DeepLearning/.venv/lib/python3.11/site-packages/optree/ops.py:766\u001B[39m, in \u001B[36mtree_map\u001B[39m\u001B[34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001B[39m\n\u001B[32m    764\u001B[39m leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)\n\u001B[32m    765\u001B[39m flat_args = [leaves] + [treespec.flatten_up_to(r) \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m rests]\n\u001B[32m--> \u001B[39m\u001B[32m766\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtreespec\u001B[49m\u001B[43m.\u001B[49m\u001B[43munflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mflat_args\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Utrecht University/Data Visualization/DeepLearning/.venv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/array_slicing.py:435\u001B[39m, in \u001B[36mconvert_to_sliceable.<locals>.convert_single_array\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m    432\u001B[39m         cast_dtype = backend.floatx()\n\u001B[32m    434\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cast_dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m435\u001B[39m     x = \u001B[43msliceable_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcast_dtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    437\u001B[39m \u001B[38;5;66;03m# Step 3. Apply target backend specific logic and optimizations.\u001B[39;00m\n\u001B[32m    438\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m target_backend \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Utrecht University/Data Visualization/DeepLearning/.venv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/array_slicing.py:63\u001B[39m, in \u001B[36mSliceable.cast\u001B[39m\u001B[34m(cls, x, dtype)\u001B[39m\n\u001B[32m     53\u001B[39m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[32m     54\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcast\u001B[39m(\u001B[38;5;28mcls\u001B[39m, x, dtype):\n\u001B[32m     55\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Cast a tensor to a different dtype.\u001B[39;00m\n\u001B[32m     56\u001B[39m \n\u001B[32m     57\u001B[39m \u001B[33;03m    Only called on a full array as provided by the user.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     61\u001B[39m \u001B[33;03m    Returns: the cast tensor.\u001B[39;00m\n\u001B[32m     62\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
